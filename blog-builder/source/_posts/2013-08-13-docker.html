---
layout: post
title: A Short Talk on Docker
tags: []
status: publish
type: post
published: true
meta: {}
---
<p>I gave a short talk on <a href="//docker.io">Docker</a>
at <a href="//phillyrb.org">PhillyRB</a>'s August
meeting.  <a href="//slideshare.net/jwatnj/docker">These were my
slides</a>, although I went way off of them and only got through about
half.</p>

<p>These are my speech notes</p>

<pre>What's Docker?

3 things I'd like you to take from this talk:
- Docker provides a lot of the advantages of virtual machines without the disadvantages
- Container-based deployment is a great way to deploy applications
- AUFS and LXC are the chocolate and peanut butter that makes Docker so great to use


- What's a container?
*It's like a virtual machine
When your app is running inside a container, it appears to be running
inside its own OS
But there could be many other containers running alongside yours,
sharing resources
It uses a combination of LXC (Linux containers) and AUFS (A
union/layered/snapshotting filesystem)
I'll talk about both more later
*They're also a means of packaging; you can build a container and run
it anywhere that can run docker
You can share that container with other people and know that it will run


- Why are they useful?

* Run your whole damn stack
Run your Rails app on your laptop by grabbing a container with RVM
installed, another container with PostgreSQL, another container with Redis,
another with ElasticSearch or Solr.
If you've ever tried to run multi-node Vagrant, it's really painful
and slow to spin up
several nodes. Docker makes it very fast and easy

* Reduce dependencies by shipping dependencies inside the container
Compile native gems or anything else once inside your container and
ship that everywhere instead of building on each server

* Cut down on configuration errors in production
Configure, build, then snapshot

* They're fast
Much faster and less resource intensive than VMs.
As we'll see in the demo, you can spin one up almost instantly.

* Deploy Once, Deploy Anywhere
Once you've built a container, and committed it as an image
you can run it anywhere

* Let someone else do the work for you
Ability to package and share containers means that you can start from
a known good ubuntu installation with RVM, like sharing chef cookbooks

* Easy to build
Make a mistake building your application, no problem,
every successful step is automatically snapshotted
If you've ever tried provisioning servers with vagrant you know you're
almost always starting over every time


- What does working with containers look like?
Not sure if this is the demo I'll be doing

Here's me grabbing a base ubuntu container
Here's me running a basic command inside of that container
Here's me grabbing the centos container
Here's me running that same command inside of the centos container

See an awesome talk about MongoDB tonight and want to try it?
docker run -i -t damien/mongodb /bin/bash
mongod
mongo
db.isMaster()

Here's me fetching and running elasticsearch with a single command

Here I am building a new Rails app and running it
Show off docker diff; docker logs; docker attach
Here I am building another one and running it alongside the first one
Here I am running 30 of these Rails app on my little MBA


- How do containers work?

LXC gives you isolation and control over sharing of resources
LXC namespaces are what makes each container appear to have its own networking,
process table, filesystem mount points, and hostname
LXC is also how you say: "Give my Rails container 4GB of RAM, but my
database container 28GB".

It gives you some control over resource management including
IO ops/second and bytes per second.

AUFS
AUFS, union or "layering" filesystem
The filesystem layers give each container its own view of the complete
filesystem
Base layer is the OS and is read-only
Your container is read-write on top
When you go to do a read, it hits your container first, then reads
through to the base if you haven't made any changes
Your container sits in a layer

Unchanged files are shared
Buffercache is also shared, so a certain portion of what's shared on
disk is shared in memory as well.
This aspect is importants because it improves performance. Storage is
cheap so who really cares that AUFS saves a few gigabytes, but the
fact that most dynamic libraries (like glibc) are shared across
containers in memory means that containers spin up very quickly


- Where can I find useful prebuilt containers?
https://index.docker.io/
Search Github for Dockerfiles

- How do I build containers?
Dockerfile, like a Gemfile but more like a shell script
You list what image you want to start from, then you list
the commands you want to run, files you want to include, ports you
want to expose

- What are the downsides?
New, doesn't feel buggy, just not a lot of good answers yet for common
ops questions like:
What happens a container crashes or the daemon crashes? How to monitor these?
Have to run your own repository to share private docker images

- What's the future?
Already several platforms like Heroku are popping up to support
running Docker containers
Docker 1.0 in the fall
Like 100 committers on it and they just opensourced it
I follow some sysadmins that are really heavy into Opscode Chef, and
Docker is changing how they ship software
Do MUCH less configuration in Chef, ship almost entirely configured containers
Test Chef recipes very quickly
Continuous Integration, test app inside lightweight container, ship
that container
</pre>
